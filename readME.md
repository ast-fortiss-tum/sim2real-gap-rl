# fortissÂ Selfâ€‘Driving Devices â€” ReinforcementÂ Learning Extension

## Overview

This repository contains **additional work** carried out at **fortiss labs** to extend the selfâ€‘driving devices research project withÂ reinforcement learning (RL) controllers and vision styleâ€‘transfer components.

Key components:

* Preâ€‘trained RL policies (TC1â€‘TC5Â + ablation)
* CycleGAN visual transfer models for simulationÂ â‡„Â camera domains
* Training scripts for the UnityÂ simulator and fakeâ€‘camera domain adaptation
* Evaluation utilities (statistics, GIF generation, result visualisation)

---

## Repository structure

```text
fortiss_rl_extension/
â”œâ”€â”€ CycleGAN/
â”‚   â”œâ”€â”€ cyclegan_model_G_AB.pth   # simulation âœ camera
â”‚   â””â”€â”€ cyclegan_model_G_BA.pth   # camera âœ simulation
â”œâ”€â”€ dataset_final/                # stored training logs & checkpoints
â”œâ”€â”€ evaluation/                   # extract tables of statistics
â”œâ”€â”€ visualization_results/        # scripts & assets for GIFs/plots
â”œâ”€â”€ create_GIF.py                 # helper â€“ combines frames to GIF
â”œâ”€â”€ training_RL.py                # RL training (simulation input)
â”œâ”€â”€ training_RL_GAN.py            # RL training (fakeâ€‘camera input)
â”œâ”€â”€ launch/
â”‚   â””â”€â”€ test_launch.launch        # example ROS launch file
â””â”€â”€ src/                          # main ROS workspace (nodes)
    â””â”€â”€ <your_node_package>/
        â”œâ”€â”€ model_RL_TC1.py       # pointâ€‘following (real data)
        â”œâ”€â”€ model_RL_TC2.py       # trained in simulation â€“ camera input
        â”œâ”€â”€ model_RL_TC3.py       # trained in simulation â€“ simâ€‘state input
        â”œâ”€â”€ model_RL_TC4.py       # trained on fake camera â€“ camera input
        â”œâ”€â”€ model_RL_TC5.py       # trained in simulation â€“ fake sim input
        â””â”€â”€ model_RL_abl.py       # ablation â€“ fake cameraÂ input
```

---

## Prerequisites

| Requirement             | Version / Notes                               |
| ----------------------- | --------------------------------------------- |
| **ROS**                 | NoeticÂ orÂ Melodic (tested on UbuntuÂ 20.04)    |
| **UnityÂ MLâ€‘Agents**     | Simulator installation already completedÂ âœ”    |
| **Python**              | â‰¥Â 3.8 with dependencies in `requirements.txt` |
| **CUDA GPU** (optional) | CUDAÂ â‰¥Â 11Â boosts training speed               |

---

## Installation

1. **Clone** inside your catkinÂ workspace:

   ```bash
   cd ~/catkin_ws/src
   git clone <this_repository_url> fortiss_rl_extension
   cd ..
   catkin_make
   ```
2. **Copy RL policy files** into the node package inside `src/` (see structure above).
3. **Create** the `CycleGAN/` directory at project root and place the two `.pth` models there.

---

## Usage

### Launch an experiment on the real robot

```bash
roslaunch fortiss_rl_extension test_launch.launch
```

In the launch file, load **exactly one** `model_RL_TC*.py` (or `model_RL_abl.py`) according to the scenario:

| Model file        | Training domain         | Runtime observation |
| ----------------- | ----------------------- | ------------------- |
| `model_RL_TC1.py` | Realâ€‘world trajectories | Waypoint vector     |
| `model_RL_TC2.py` | Unity simulation        | Camera              |
| `model_RL_TC3.py` | Unity simulation        | Simulation state    |
| `model_RL_TC4.py` | Fake camera (CycleGAN)  | Camera              |
| `model_RL_TC5.py` | Unity simulation        | Fake sim state      |
| `model_RL_abl.py` | Fake camera             | Fake camera         |

> **Safety checklist** ğŸ›¡ï¸
> â€¢ Battery â‰¥Â 11.5â€¯V
> â€¢ Stable Wiâ€‘Fi connection to ROSÂ master
> â€¢ Obstacleâ€‘free testing area

---

### Train a new policy

| Script               | Observation        | Notes                                |
| -------------------- | ------------------ | ------------------------------------ |
| `training_RL.py`     | Simulation state   | Opens UnityÂ executable automatically |
| `training_RL_GAN.py` | Fake camera frames | Requires preâ€‘trained CycleGAN        |

---

### Evaluate & visualise results

```bash
python evaluation/evaluate.py --runs dataset_final
python create_GIF.py --input dataset_final/run42
```

The scripts deposit tables, GIFs and summary graphics in `visualization_results/`.

---

## Results

`dataset_final/` already contains the runs referenced in the accompanying Master Thesis.

| Experiment | SuccessÂ rate | Avg.Â reward |
| ---------- | ------------ | ----------- |
| TC1        | 94â€¯%         | 235Â Â±â€¯12    |
| TC2        | 87â€¯%         | 210Â Â±â€¯15    |
| TC3        | *â€¦*          | *â€¦*         |

Full statistics can be regenerated with the `evaluation/` utilities.

---

## Citation

If you build on this work, please cite:

> **Author Name**, â€œDomainâ€‘Invariant Reinforcement Learning for Selfâ€‘Driving Devices,â€ Master Thesis, fortiss labs, TechnicalÂ University ofÂ Munich, 2025.

---

## License

Distributed under the MITÂ License. See `LICENSE` for details.

---

## Contact

Questions or improvements? Open an issue or pullÂ requestÂ âœ‰ï¸.
